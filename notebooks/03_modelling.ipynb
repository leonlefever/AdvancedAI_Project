{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f39ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created listings_with_preds.csv with 21707 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your real dataset\n",
    "df = pd.read_csv(\"../data/processed/cleaned_data.csv\")\n",
    "\n",
    "# Create the table for embeddings\n",
    "df_to_embed = pd.DataFrame({\n",
    "    \"listing_id\": range(len(df)),  # Generate unique ID from index\n",
    "    \"title\": df[\"title\"],\n",
    "    \"subtitle\": df[\"subtitle\"],\n",
    "    \"pred_price\": df[\"buy_price\"],  # Use actual price\n",
    "    \"shap_top3\": \"[]\"               # placeholder for now\n",
    "})\n",
    "\n",
    "# Save it\n",
    "df_to_embed.to_csv(\"../data/processed/listings_with_preds.csv\", index=False)\n",
    "print(\"Created listings_with_preds.csv with\", len(df_to_embed), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5706ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 0.222\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "for col in [\"neighborhood_id\", \"house_type_id\"]:\n",
    "    df[col] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"sq_mt_built\", \"n_rooms\", \"n_bathrooms\", \"neighborhood_id\", \n",
    "    \"house_type_id\", \"has_terrace\", \"has_lift\", \"is_exterior\",\n",
    "    \"log_sq_mt_built\", \"building_age\"\n",
    "]\n",
    "target = \"log_buy_price\"\n",
    "\n",
    "# Prepare data\n",
    "df_model = df[features + [target]].dropna()\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost regressor\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "print(\"XGBoost RMSE:\", round(rmse, 3))\n",
    "\n",
    "# Predict on full data and save\n",
    "df[\"pred_price\"] = np.expm1(model.predict(df[features].fillna(0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Obtaining dependency information for shap from https://files.pythonhosted.org/packages/e5/bb/dc75933de86e6076f58cf68325877be952a97a371c26b252013f1258a5a7/shap-0.47.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading shap-0.47.2-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (4.66.4)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (24.2)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Obtaining dependency information for slicer==0.0.8 from https://files.pythonhosted.org/packages/63/81/9ef641ff4e12cbcca30e54e72fb0951a2ba195d0cda0ba4100e532d929db/slicer-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Obtaining dependency information for numba>=0.54 from https://files.pythonhosted.org/packages/0f/a4/2b309a6a9f6d4d8cfba583401c7c2f9ff887adb5d54d8e2e130274c0973f/numba-0.61.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numba-0.61.2-cp311-cp311-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Obtaining dependency information for cloudpickle from https://files.pythonhosted.org/packages/7e/e8/64c37fadfc2816a7701fa8a6ed8d87327c7d54eacfbfb6edab14a2f2be75/cloudpickle-3.1.1-py3-none-any.whl.metadata\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from shap) (4.13.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Obtaining dependency information for llvmlite<0.45,>=0.44.0dev0 from https://files.pythonhosted.org/packages/5f/c6/258801143975a6d09a373f2641237992496e15567b907a4d401839d671b8/llvmlite-0.44.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\leon\\appdata\\roaming\\python\\python311\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\leon\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leon\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.47.2-cp311-cp311-win_amd64.whl (544 kB)\n",
      "   ---------------------------------------- 0.0/544.4 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 112.6/544.4 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 225.3/544.4 kB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 358.4/544.4 kB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 501.8/544.4 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 544.4/544.4 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading numba-0.61.2-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/2.8 MB 3.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.6/2.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.8/2.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/2.8 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.1/2.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.5/2.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.7/2.8 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.9/2.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.2/2.8 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.4/2.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.7/2.8 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 3.8 MB/s eta 0:00:00\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/30.3 MB 5.4 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.5/30.3 MB 5.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.8/30.3 MB 5.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.1/30.3 MB 5.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.4/30.3 MB 5.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.7/30.3 MB 6.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.1/30.3 MB 6.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.5/30.3 MB 6.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.9/30.3 MB 6.8 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 3.3/30.3 MB 7.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 4.0/30.3 MB 7.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 4.5/30.3 MB 7.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 5.0/30.3 MB 8.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 5.4/30.3 MB 8.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 6.2/30.3 MB 8.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 7.0/30.3 MB 9.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 7.7/30.3 MB 9.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 8.5/30.3 MB 10.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 9.2/30.3 MB 10.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 10.0/30.3 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 10.6/30.3 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 11.5/30.3 MB 12.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 12.0/30.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 12.9/30.3 MB 14.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 13.5/30.3 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 14.3/30.3 MB 15.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 15.4/30.3 MB 16.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 16.2/30.3 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 17.3/30.3 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 18.3/30.3 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 19.1/30.3 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 20.4/30.3 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 21.9/30.3 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 23.2/30.3 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 24.3/30.3 MB 23.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 25.7/30.3 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 26.9/30.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 27.6/30.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 28.6/30.3 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  29.9/30.3 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.3/30.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 21.8 MB/s eta 0:00:00\n",
      "Installing collected packages: slicer, llvmlite, cloudpickle, numba, shap\n",
      "Successfully installed cloudpickle-3.1.1 llvmlite-0.44.0 numba-0.61.2 shap-0.47.2 slicer-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "SHAP explanations added and saved\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import json\n",
    "\n",
    "# 1. Create SHAP explainer\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# 2. Get top-3 features per listing (by absolute SHAP value)\n",
    "top3_idxs = np.argsort(-np.abs(shap_values.values), axis=1)[:, :3]\n",
    "top3_features = [[features[i] for i in row] for row in top3_idxs]\n",
    "\n",
    "# 3. Reload the listings_with_preds file\n",
    "df_preds = pd.read_csv(\"../data/processed/listings_with_preds.csv\")\n",
    "\n",
    "# 4. Add SHAP top 3 as a stringified JSON list\n",
    "df_preds[\"shap_top3\"] = [json.dumps(lst) for lst in top3_features]\n",
    "\n",
    "# 5. Save it again\n",
    "df_preds.to_csv(\"../data/processed/listings_with_preds.csv\", index=False)\n",
    "print(\"SHAP explanations added and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b62f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c01af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 21707 embeddings to vectorstore/\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd, json, numpy as np, pathlib\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/listings_with_preds.csv\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "texts, ids = [], []\n",
    "for _, r in df.iterrows():\n",
    "    shap_list = json.loads(r[\"shap_top3\"])  # even if empty\n",
    "    txt = (\n",
    "        f\"Title: {r['title']}\\n\"\n",
    "        f\"Neighbourhood: {r['subtitle']}\\n\"\n",
    "        f\"Top-3 price drivers: {', '.join(shap_list)}\\n\"\n",
    "        f\"Predicted price: ‚Ç¨{r['pred_price']:,.0f}\"\n",
    "    )\n",
    "    texts.append(txt)\n",
    "    ids.append(int(r[\"listing_id\"]))\n",
    "\n",
    "emb = model.encode(texts, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "pathlib.Path(\"vectorstore\").mkdir(exist_ok=True)\n",
    "np.save(\"vectorstore/embeddings.npy\", emb)\n",
    "np.save(\"vectorstore/ids.npy\", np.array(ids))\n",
    "\n",
    "print(\"Saved\", emb.shape[0], \"embeddings to vectorstore/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55d025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pathlib\n",
    "\n",
    "# texts = []\n",
    "# ids   = []\n",
    "\n",
    "# for _, r in df.iterrows():\n",
    "#     shap_list = json.loads(r[\"shap_top3\"])\n",
    "#     txt = (\n",
    "#         f\"Title: {r['title']}\\n\"\n",
    "#         f\"Neighbourhood: {r['subtitle']}\\n\"\n",
    "#         f\"Top-3 price drivers: {', '.join(shap_list)}\\n\"\n",
    "#         f\"Predicted price: ‚Ç¨{r['pred_price']:,.0f}\"\n",
    "#     )\n",
    "#     texts.append(txt)\n",
    "#     ids.append(int(r[\"listing_id\"]))\n",
    "\n",
    "# emb_matrix = model.encode(texts, convert_to_numpy=True).astype(\"float32\")   # shape (N, 384)\n",
    "\n",
    "# # Save\n",
    "# pathlib.Path(\"vectorstore\").mkdir(exist_ok=True)\n",
    "# np.save(\"vectorstore/embeddings.npy\", emb_matrix)\n",
    "# np.save(\"vectorstore/ids.npy\", np.array(ids))\n",
    "\n",
    "# print(\"Saved\", emb_matrix.shape[0], \"embeddings ‚Üí vectorstore/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e748540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load previously saved embeddings and index\n",
    "emb = np.load(\"vectorstore/embeddings.npy\")\n",
    "ids = np.load(\"vectorstore/ids.npy\")\n",
    "index = faiss.read_index(\"vectorstore/madrid.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4ecebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 matching listing IDs: [14277 15712 14856 15021 14863]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "query = \"modern flat with terrace in chamart√≠n\"\n",
    "q_vec = model.encode(query, convert_to_numpy=True).astype(\"float32\")\n",
    "faiss.normalize_L2(q_vec.reshape(1, -1))\n",
    "\n",
    "D, I = index.search(q_vec.reshape(1, -1), k=5)\n",
    "print(\"Top 5 matching listing IDs:\", ids[I[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71cdc8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_11988\\3209062961.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load your data\n",
    "df = pd.read_csv(\"../data/processed/listings_with_preds.csv\")\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=f\"Listing {row.listing_id}: {row.title} in {row.subtitle}, price ‚Ç¨{row.pred_price}\",\n",
    "        metadata={\"listing_id\": int(row.listing_id)}\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# 2. Use local model to avoid OpenAI embedding mismatch\n",
    "hf_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "# 3. Build the new vectorstore from scratch\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "231aed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leon\\AppData\\Local\\Temp\\ipykernel_11988\\184841102.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Homes in Chamart√≠n may be more expensive due to factors such as the neighborhood's location, amenities, infrastructure, demand, and overall desirability. Chamart√≠n is a well-established and sought-after neighborhood in Madrid, known for its upscale properties, proximity to the city center, good transportation connections, green spaces, and quality of life. These factors can contribute to higher property prices compared to other areas.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "response = qa.invoke(\"Why are homes in Chamart√≠n more expensive?\")\n",
    "print(\"ü§ñ\", response[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
