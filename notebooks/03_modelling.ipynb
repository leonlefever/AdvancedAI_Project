{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f39ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created listings_with_preds.csv with 21707 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/processed/cleaned_data.csv\")\n",
    "\n",
    "# Create the table for embeddings\n",
    "df_to_embed = pd.DataFrame({\n",
    "    \"listing_id\": range(len(df)),  # Generate unique ID from index\n",
    "    \"title\": df[\"title\"],\n",
    "    \"subtitle\": df[\"subtitle\"],\n",
    "    \"pred_price\": df[\"buy_price\"],  # Use actual price\n",
    "    \"shap_top3\": \"[]\"               # placeholder for now\n",
    "})\n",
    "\n",
    "# Save it\n",
    "df_to_embed.to_csv(\"../data/processed/listings_with_preds.csv\", index=False)\n",
    "print(\"Created listings_with_preds.csv with\", len(df_to_embed), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5706ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 0.222\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "for col in [\"neighborhood_id\", \"house_type_id\"]:\n",
    "    df[col] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"sq_mt_built\", \"n_rooms\", \"n_bathrooms\", \"neighborhood_id\", \n",
    "    \"house_type_id\", \"has_terrace\", \"has_lift\", \"is_exterior\",\n",
    "    \"log_sq_mt_built\", \"building_age\"\n",
    "]\n",
    "target = \"log_buy_price\"\n",
    "\n",
    "# Prepare data\n",
    "df_model = df[features + [target]].dropna()\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost regressor\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "print(\"XGBoost RMSE:\", round(rmse, 3))\n",
    "\n",
    "# Predict on full data and save\n",
    "df[\"pred_price\"] = np.expm1(model.predict(df[features].fillna(0)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21dd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP explanations added and saved\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import json\n",
    "\n",
    "# 1. Create SHAP explainer\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# 2. Get top-3 features per listing (by absolute SHAP value)\n",
    "top3_idxs = np.argsort(-np.abs(shap_values.values), axis=1)[:, :3]\n",
    "top3_features = [[features[i] for i in row] for row in top3_idxs]\n",
    "\n",
    "# 3. Reload the listings_with_preds file\n",
    "df_preds = pd.read_csv(\"../data/processed/listings_with_preds.csv\")\n",
    "\n",
    "# 4. Add SHAP top 3 as a stringified JSON list\n",
    "df_preds[\"shap_top3\"] = [json.dumps(lst) for lst in top3_features]\n",
    "\n",
    "# 5. Save it again\n",
    "df_preds.to_csv(\"../data/processed/listings_with_preds.csv\", index=False)\n",
    "print(\"SHAP explanations added and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b62f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c01af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 21707 embeddings to ../vectorstore\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd, json, numpy as np, pathlib\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/listings_with_preds.csv\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "texts, ids = [], []\n",
    "for _, r in df.iterrows():\n",
    "    shap_list = json.loads(r[\"shap_top3\"])  # even if empty\n",
    "    txt = (\n",
    "        f\"Title: {r['title']}\\n\"\n",
    "        f\"Neighbourhood: {r['subtitle']}\\n\"\n",
    "        f\"Top-3 price drivers: {', '.join(shap_list)}\\n\"\n",
    "        f\"Predicted price: ‚Ç¨{r['pred_price']:,.0f}\"\n",
    "    )\n",
    "    texts.append(txt)\n",
    "    ids.append(int(r[\"listing_id\"]))\n",
    "\n",
    "emb = model.encode(texts, convert_to_numpy=True).astype(\"float32\")\n",
    "\n",
    "output_dir = \"../vectorstore\"\n",
    "pathlib.Path(output_dir).mkdir(exist_ok=True)\n",
    "np.save(f\"{output_dir}/embeddings.npy\", emb)\n",
    "np.save(f\"{output_dir}/ids.npy\", np.array(ids))\n",
    "\n",
    "print(\"Saved\", emb.shape[0], \"embeddings to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e582bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to ../vectorstore/madrid.faiss\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "faiss.write_index(index, \"../vectorstore/madrid.faiss\")\n",
    "print(\"FAISS index saved to ../vectorstore/madrid.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e748540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "output_dir = \"../vectorstore\"\n",
    "\n",
    "# Load saved embeddings and index\n",
    "emb = np.load(f\"{output_dir}/embeddings.npy\")\n",
    "ids = np.load(f\"{output_dir}/ids.npy\")\n",
    "index = faiss.read_index(f\"{output_dir}/madrid.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ecebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 matching listing IDs: [15712 15404 15021 15469 14339]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "query = \"modern flat with terrace in chamart√≠n\"\n",
    "q_vec = model.encode(query, convert_to_numpy=True).astype(\"float32\")\n",
    "faiss.normalize_L2(q_vec.reshape(1, -1))\n",
    "\n",
    "D, I = index.search(q_vec.reshape(1, -1), k=5)\n",
    "print(\"Top 5 matching listing IDs:\", ids[I[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71cdc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/listings_with_preds.csv\")\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=f\"Listing {row.listing_id}: {row.title} in {row.subtitle}, price ‚Ç¨{row.pred_price}\",\n",
    "        metadata={\"listing_id\": int(row.listing_id)}\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "\n",
    "hf_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "231aed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Homes in Chamart√≠n may be more expensive due to factors such as the neighborhood's location, amenities, infrastructure, demand, and overall desirability. Additionally, specific features of the properties, such as size, condition, and unique characteristics, can also influence the pricing.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "response = qa.invoke(\"Why are homes in Chamart√≠n more expensive?\")\n",
    "print(\"ü§ñ\", response[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
